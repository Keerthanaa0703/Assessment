# -*- coding: utf-8 -*-
"""LVADSUSR93-Keerthanaa-Lab-1-IA2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MGqC_Rs1_4P3Or_WLAMM-_lTZxRIjIRv
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix
from sklearn.impute import SimpleImputer
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
data = pd.read_csv("/content/winequality-red.csv")

print(data.head())
print(data.info())

# Checking missing values
print(data.isnull().sum())

# Manage outliers
Q1 = data.quantile(0.25)
Q3 = data.quantile(0.75)
IQR = Q3 - Q1
outliers = ((data < (Q1 - 1.5 * IQR)) | (data > (Q3 + 1.5 * IQR))).any(axis=1)
cleaned_df = data[~outliers]
cleaned_df
plt.figure(figsize=(10, 6))
sns.boxplot(data=cleaned_df[['density', 'pH', 'volatile acidity', 'fixed acidity']])
plt.title('Boxplot for Outlier Detection')
plt.show()

# Data Transformation
data['quality'] = data['quality'].apply(lambda x: 'Good' if x in (7, 8) else 'Bad')

# Converting categorical variables into numeric format
le = LabelEncoder()
data['quality'] = le.fit_transform(data['quality'])

# Feature Selection and Data Cleaning
selected_features = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',
                     'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',
                     'pH', 'sulphates', 'alcohol']

# Removing duplicates
data.drop_duplicates()

# Data Splitting
X = data[selected_features]
y = data['quality']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Handle missing values
imputer = SimpleImputer(strategy='mean')
X_train_imputed = imputer.fit_transform(X_train)
X_test_imputed = imputer.transform(X_test)

# Model Development- K-NN classifier
knn_classifier = KNeighborsClassifier(n_neighbors=5)
knn_classifier.fit(X_train_imputed, y_train)

#Random Forest classifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)
rf_classifier.fit(X_train_imputed, y_train)

#Model Evaluation
#K-NN Model Evaluation
knn_pred = knn_classifier.predict(X_test_imputed)
knn_accuracy = accuracy_score(y_test, knn_pred)
knn_precision = precision_score(y_test, knn_pred)
knn_recall = recall_score(y_test, knn_pred)
knn_confusion_matrix = confusion_matrix(y_test, knn_pred)

#Random Forest Model Evaluation
rf_pred = rf_classifier.predict(X_test_imputed)
rf_accuracy = accuracy_score(y_test, rf_pred)
rf_precision = precision_score(y_test, rf_pred)
rf_recall = recall_score(y_test, rf_pred)
rf_confusion_matrix = confusion_matrix(y_test, rf_pred)

# Print evaluation metrics
print("K-Nearest Neighbors Classifier:")
print("Accuracy:", knn_accuracy)
print("Precision:", knn_precision)
print("Recall:", knn_recall)
print("Confusion Matrix:")
print(knn_confusion_matrix)

print("\nRandom Forest Classifier:")
print("Accuracy:", rf_accuracy)
print("Precision:", rf_precision)
print("Recall:", rf_recall)
print("Confusion Matrix:")
print(rf_confusion_matrix)